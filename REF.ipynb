{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "U0WyTi6xInDs",
        "eCZUpgzrJcKg",
        "EYt_aMYat0S1",
        "h7I7UUkfuY8Y",
        "dj_wsXapxLft"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install and Import Libraries"
      ],
      "metadata": {
        "id": "ewKuy4rI2xBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "0UF_mgytLSSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTLYN2rXymP2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb2056a2-bbe4-462c-96a3-486b00989bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.metrics import pairwise_distances_argmin_min,classification_report, accuracy_score\n",
        "from textblob import TextBlob\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_val_predict\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from transformers import BertModel,BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments,DataCollatorWithPadding\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from keras.layers import GRU\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, SpatialDropout1D\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# Visualize missing values\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import gensim.downloader as api"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download NLTK Packages"
      ],
      "metadata": {
        "id": "U0WyTi6xInDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "XZjTJgwwIrSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "KQl5MVOMI15w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize BERT tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "NpaMQ_9eJZLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize stopwords and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "SGoDthslJVNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Pre-process Data"
      ],
      "metadata": {
        "id": "eCZUpgzrJcKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file\n",
        "df = pd.read_csv('Impact.csv',encoding='latin1')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Y76cItqmJbp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define text processing functions\n",
        "def clean_text(text):\n",
        "    text = str(text)  # Ensure the input is a string\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespaces\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove text inside brackets\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation and numbers\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    return text"
      ],
      "metadata": {
        "id": "Xq-QTc5ULqkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(filtered_words)"
      ],
      "metadata": {
        "id": "TTE2-99ULtUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_text(text):\n",
        "    words = word_tokenize(text)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(lemmatized_words)"
      ],
      "metadata": {
        "id": "z-0lHy3_LvWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)"
      ],
      "metadata": {
        "id": "oM0Wi3buLxao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def named_entity_recognition(text):\n",
        "    doc = nlp(text)\n",
        "    return [(ent.text, ent.label_) for ent in doc.ents]"
      ],
      "metadata": {
        "id": "pClI8WrWL8KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_tagging(text):\n",
        "    words = word_tokenize(text)\n",
        "    return nltk.pos_tag(words)"
      ],
      "metadata": {
        "id": "Kyn780YLL-Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(text, method='tfidf'):\n",
        "    if method == 'tfidf':\n",
        "        vectorizer = TfidfVectorizer()\n",
        "    elif method == 'bow':\n",
        "        vectorizer = CountVectorizer()\n",
        "    else:\n",
        "        raise ValueError(\"Method must be 'tfidf' or 'bow'\")\n",
        "\n",
        "    return vectorizer.fit_transform(text)"
      ],
      "metadata": {
        "id": "HaU7HfEbMCG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_embedding(text):\n",
        "    inputs = bert_tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()"
      ],
      "metadata": {
        "id": "HjjCuueNME0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_contractions(text):\n",
        "    contractions_dict = {\n",
        "        \"can't\": \"cannot\",\n",
        "        \"won't\": \"will not\",\n",
        "        \"n't\": \" not\",\n",
        "        \"'re\": \" are\",\n",
        "        \"'s\": \" is\",\n",
        "        \"'d\": \" would\",\n",
        "        \"'ll\": \" will\",\n",
        "        \"'t\": \" not\",\n",
        "        \"'ve\": \" have\",\n",
        "        \"'m\": \" am\"\n",
        "    }\n",
        "    contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)"
      ],
      "metadata": {
        "id": "Oa5nkERgMUSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_spelling(text):\n",
        "    return str(TextBlob(text).correct())"
      ],
      "metadata": {
        "id": "RPAUPXZCMXJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_missing_data(text):\n",
        "    if pd.isnull(text):\n",
        "        return \"missing\"\n",
        "    return text"
      ],
      "metadata": {
        "id": "velfU_i4MZeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file\n",
        "df = pd.read_csv('Impact_cleaned_data.csv',encoding='latin1')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "KWsZCkeqMccj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "EYt_aMYat0S1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate text length for each column\n",
        "df['Title_length'] = df['Title'].apply(lambda x: len(str(x).split()))\n",
        "df['Summary_length'] = df['1. Summary of the impact'].apply(lambda x: len(str(x).split()))\n",
        "df['Research_length'] = df['2. Underpinning research'].apply(lambda x: len(str(x).split()))\n",
        "df['References_length'] = df['3. References to the research'].apply(lambda x: len(str(x).split()))\n",
        "df['Details_length'] = df['4. Details of the impact'].apply(lambda x: len(str(x).split()))\n",
        "df['Sources_length'] = df['5. Sources to corroborate the impact'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# Summary statistics for text lengths\n",
        "print(df[['Title_length', 'Summary_length', 'Research_length', 'References_length', 'Details_length', 'Sources_length']].describe())\n",
        "\n",
        "# Visualize text length distributions\n",
        "df[['Title_length', 'Summary_length', 'Research_length', 'References_length', 'Details_length', 'Sources_length']].hist(bins=30, figsize=(15, 10))\n",
        "plt.suptitle('Text Length Distributions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H_wWko4yt9d7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocabulary_size(text_series):\n",
        "    all_words = ' '.join(text_series).split()\n",
        "    unique_words = set(all_words)\n",
        "    return len(unique_words)\n",
        "\n",
        "# Get vocabulary size for each column\n",
        "title_vocab_size = get_vocabulary_size(df['Title'])\n",
        "summary_vocab_size = get_vocabulary_size(df['1. Summary of the impact'])\n",
        "research_vocab_size = get_vocabulary_size(df['2. Underpinning research'])\n",
        "references_vocab_size = get_vocabulary_size(df['3. References to the research'])\n",
        "details_vocab_size = get_vocabulary_size(df['4. Details of the impact'])\n",
        "sources_vocab_size = get_vocabulary_size(df['5. Sources to corroborate the impact'])\n",
        "\n",
        "print(\"Vocabulary size in Titles:\", title_vocab_size)\n",
        "print(\"Vocabulary size in Summaries:\", summary_vocab_size)\n",
        "print(\"Vocabulary size in Research:\", research_vocab_size)\n",
        "print(\"Vocabulary size in References:\", references_vocab_size)\n",
        "print(\"Vocabulary size in Details:\", details_vocab_size)\n",
        "print(\"Vocabulary size in Sources:\", sources_vocab_size)"
      ],
      "metadata": {
        "id": "bv4K4fhquBmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to get word frequency\n",
        "def get_word_frequency(text_series):\n",
        "    all_words = ' '.join(text_series).split()\n",
        "    all_words = [word for word in all_words if word not in stop_words]\n",
        "    word_freq = Counter(all_words)\n",
        "    return word_freq\n",
        "\n",
        "# Get word frequencies for each column\n",
        "title_word_freq = get_word_frequency(df['Title'])\n",
        "summary_word_freq = get_word_frequency(df['1. Summary of the impact'])\n",
        "research_word_freq = get_word_frequency(df['2. Underpinning research'])\n",
        "references_word_freq = get_word_frequency(df['3. References to the research'])\n",
        "details_word_freq = get_word_frequency(df['4. Details of the impact'])\n",
        "sources_word_freq = get_word_frequency(df['5. Sources to corroborate the impact'])\n",
        "\n",
        "# Print top 10 common words\n",
        "print(\"Top 10 common words in Titles:\", title_word_freq.most_common(10))\n",
        "print(\"Top 10 common words in Summaries:\", summary_word_freq.most_common(10))\n",
        "print(\"Top 10 common words in Research:\", research_word_freq.most_common(10))\n",
        "print(\"Top 10 common words in References:\", references_word_freq.most_common(10))\n",
        "print(\"Top 10 common words in Details:\", details_word_freq.most_common(10))\n",
        "print(\"Top 10 common words in Sources:\", sources_word_freq.most_common(10))"
      ],
      "metadata": {
        "id": "hCAQ1InmuE5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get n-grams\n",
        "def get_ngrams(text_series, n=2):\n",
        "    vectorizer = CountVectorizer(ngram_range=(n, n))\n",
        "    X = vectorizer.fit_transform(text_series)\n",
        "    ngram_counts = X.sum(axis=0)\n",
        "    ngram_freq = [(word, ngram_counts[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
        "    ngram_freq = sorted(ngram_freq, key=lambda x: x[1], reverse=True)\n",
        "    return ngram_freq\n",
        "\n",
        "# Get bigrams for each column\n",
        "title_bigrams = get_ngrams(df['Title'], n=2)\n",
        "summary_bigrams = get_ngrams(df['1. Summary of the impact'], n=2)\n",
        "research_bigrams = get_ngrams(df['2. Underpinning research'], n=2)\n",
        "references_bigrams = get_ngrams(df['3. References to the research'], n=2)\n",
        "details_bigrams = get_ngrams(df['4. Details of the impact'], n=2)\n",
        "sources_bigrams = get_ngrams(df['5. Sources to corroborate the impact'], n=2)\n",
        "\n",
        "# Print top 10 bigrams\n",
        "print(\"Top 10 bigrams in Titles:\", title_bigrams[:10])\n",
        "print(\"Top 10 bigrams in Summaries:\", summary_bigrams[:10])\n",
        "print(\"Top 10 bigrams in Research:\", research_bigrams[:10])\n",
        "print(\"Top 10 bigrams in References:\", references_bigrams[:10])\n",
        "print(\"Top 10 bigrams in Details:\", details_bigrams[:10])\n",
        "print(\"Top 10 bigrams in Sources:\", sources_bigrams[:10])\n"
      ],
      "metadata": {
        "id": "04L_Kw2guIY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_word_freq(word_freq, title):\n",
        "    words, counts = zip(*word_freq.most_common(10))\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(words, counts)\n",
        "    plt.title(title)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Plot top 10 words\n",
        "plot_word_freq(title_word_freq, 'Top 10 Words in Titles')\n",
        "plot_word_freq(summary_word_freq, 'Top 10 Words in Summaries')\n",
        "plot_word_freq(research_word_freq, 'Top 10 Words in Research')\n",
        "plot_word_freq(references_word_freq, 'Top 10 Words in References')\n",
        "plot_word_freq(details_word_freq, 'Top 10 Words in Details')\n",
        "plot_word_freq(sources_word_freq, 'Top 10 Words in Sources')\n",
        "\n",
        "def plot_ngram_freq(ngram_freq, title):\n",
        "    ngrams, counts = zip(*ngram_freq[:10])\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(ngrams, counts)\n",
        "    plt.title(title)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Plot top 10 bigrams\n",
        "plot_ngram_freq(title_bigrams, 'Top 10 Bigrams in Titles')\n",
        "plot_ngram_freq(summary_bigrams, 'Top 10 Bigrams in Summaries')\n",
        "plot_ngram_freq(research_bigrams, 'Top 10 Bigrams in Research')\n",
        "plot_ngram_freq(references_bigrams, 'Top 10 Bigrams in References')\n",
        "plot_ngram_freq(details_bigrams, 'Top 10 Bigrams in Details')\n",
        "plot_ngram_freq(sources_bigrams, 'Top 10 Bigrams in Sources')"
      ],
      "metadata": {
        "id": "GzLzDEaIuL34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embeddings"
      ],
      "metadata": {
        "id": "h7I7UUkfuY8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all text columns into a single text column for feature extraction\n",
        "df['combined_text'] = df['Title'] + ' ' + df['1. Summary of the impact'] + ' ' + df['2. Underpinning research'] + ' ' + df['3. References to the research'] + ' ' + df['4. Details of the impact'] + ' ' + df['5. Sources to corroborate the impact']"
      ],
      "metadata": {
        "id": "ZNiArTwHubPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained word2vec model\n",
        "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "def get_average_word2vec(text, model, vector_size=300):\n",
        "    words = text.split()\n",
        "    word_vectors = [model[word] for word in words if word in model]\n",
        "    if len(word_vectors) == 0:\n",
        "        return np.zeros(vector_size)\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "df['word2vec'] = df['combined_text'].apply(lambda x: get_average_word2vec(x, word2vec_model))\n",
        "\n",
        "# Convert to numpy array\n",
        "word2vec_features = np.vstack(df['word2vec'].values)\n",
        "\n",
        "print(\"Word2Vec Features Shape:\", word2vec_features.shape)\n",
        "\n",
        "# Convert Word2Vec features into a DataFrame\n",
        "word2vec_df = pd.DataFrame(df['word2vec'].to_list(), columns=[f'w2v_{i}' for i in range(300)])\n",
        "\n",
        "# Display Word2Vec features\n",
        "print(\"Word2Vec Features:\\n\", word2vec_df.head())"
      ],
      "metadata": {
        "id": "dNwBApr4L8mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def get_bert_embedding(text, tokenizer, model):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
        "    outputs = model(**inputs)\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "    return torch.mean(last_hidden_states, dim=1).detach().numpy()\n",
        "\n",
        "df['bert'] = df['combined_text'].apply(lambda x: get_bert_embedding(x, tokenizer, model))\n",
        "\n",
        "# Convert to numpy array\n",
        "bert_features = np.vstack(df['bert'].values)\n",
        "\n",
        "print(\"BERT Features Shape:\", bert_features.shape)\n",
        "\n",
        "# Convert BERT features into a DataFrame\n",
        "bert_df = pd.DataFrame(df['bert'].to_list(), columns=[f'bert_{i}' for i in range(model.config.hidden_size)])\n",
        "\n",
        "# Display BERT features\n",
        "print(\"BERT Embeddings:\\n\", bert_df.head())"
      ],
      "metadata": {
        "id": "w3Ds8Cw0L_Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Squeeze the singleton dimension\n",
        "bert_features = np.squeeze(np.array(df['bert'].to_list()), axis=1)\n",
        "\n",
        "# Convert squeezed BERT features into a DataFrame\n",
        "bert_df = pd.DataFrame(bert_features, columns=[f'bert_{i}' for i in range(model.config.hidden_size)])\n",
        "\n",
        "# Display BERT features\n",
        "print(\"BERT Features Shape:\", bert_df.shape)\n",
        "print(\"BERT Features:\\n\", bert_df.head())"
      ],
      "metadata": {
        "id": "PkxBiDULMCZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['doc'] = df['combined_text'].apply(nlp)\n",
        "\n",
        "# Extract Named Entities, POS tags, and Dependency Parsing\n",
        "def extract_ner(doc):\n",
        "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "def extract_pos_tags(doc):\n",
        "    return [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "def extract_dependencies(doc):\n",
        "    return [(token.text, token.dep_, token.head.text) for token in doc]\n",
        "\n",
        "df['named_entities'] = df['doc'].apply(extract_ner)\n",
        "df['pos_tags'] = df['doc'].apply(extract_pos_tags)\n",
        "df['dependencies'] = df['doc'].apply(extract_dependencies)"
      ],
      "metadata": {
        "id": "eOHLXbKNvBHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['combined_text'])\n",
        "\n",
        "# Convert TF-IDF matrix to a DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Display TF-IDF features\n",
        "print(\"TF-IDF Features:\\n\", tfidf_df.head())\n",
        "\n",
        "# Convert TF-IDF matrix to dense format\n",
        "dense_matrix = tfidf_matrix.toarray()\n",
        "\n",
        "# Create a DataFrame for TF-IDF features\n",
        "tfidf_df = pd.DataFrame(dense_matrix, columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Add the target variable 'overall_rating'\n",
        "tfidf_df['overall_rating'] = df['overall_rating']\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = tfidf_df.corr()\n",
        "\n",
        "# Extract correlations of 'overall_rating' with other features\n",
        "overall_rating_correlations = correlation_matrix['overall_rating'].sort_values(ascending=False)\n",
        "\n",
        "# Plotting the top correlations\n",
        "top_correlations = overall_rating_correlations.head(10)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_correlations.values, y=top_correlations.index)\n",
        "plt.title('Top 10 Correlations with Overall Rating')\n",
        "plt.xlabel('Correlation Coefficient')\n",
        "plt.ylabel('Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zvj2Z6aNMFe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all features into a single DataFrame\n",
        "features_df = pd.concat([tfidf_df, word2vec_df, bert_df], axis=1)\n",
        "\n",
        "# The combined features_df now contains TF-IDF, Word2Vec, and BERT embeddings.\n",
        "print(\"Combined Feature DataFrame:\\n\", features_df.head())\n"
      ],
      "metadata": {
        "id": "dbVRQqy-MItQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add REF scores to the features DataFrame for correlation analysis\n",
        "features_df['overall_rating'] = df['overall_rating']\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = features_df.corr()\n",
        "\n",
        "# Visualize the correlation matrix\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Matrix of Features with REF Scores')\n",
        "plt.show()\n",
        "\n",
        "# Identify top features correlated with REF score\n",
        "top_correlated_features = correlation_matrix['overall_rating'].sort_values(ascending=False).head(10)\n",
        "print(\"Top 10 Features Correlated with REF Score:\\n\", top_correlated_features)"
      ],
      "metadata": {
        "id": "drJ2209tMLqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute correlation matrix\n",
        "# correlation_matrix = features_df.corr()\n",
        "\n",
        "# Get the top 10 features most positively and negatively correlated with REF score\n",
        "top_correlated_features = correlation_matrix['overall_rating'].abs().sort_values(ascending=False).head(11).index\n",
        "\n",
        "# Create a focused correlation matrix for these features\n",
        "focused_corr_matrix = correlation_matrix.loc[top_correlated_features, top_correlated_features]\n",
        "\n",
        "# Plot the focused correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(focused_corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Top Correlated Features with REF Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b0ZCkF14MN9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assign Proxy Ratings"
      ],
      "metadata": {
        "id": "dj_wsXapxLft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Topic Modeling\n",
        "def topic_modeling(text, num_topics=5):\n",
        "    vectorizer = CountVectorizer(stop_words='english')\n",
        "    doc_term_matrix = vectorizer.fit_transform([text])\n",
        "    LDA = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "    LDA.fit(doc_term_matrix)\n",
        "    topics = LDA.components_\n",
        "    coherence_scores = []\n",
        "    for topic in topics:\n",
        "        top_indices = topic.argsort()[-10:]\n",
        "        top_terms = [vectorizer.get_feature_names_out()[i] for i in top_indices]\n",
        "        term_vectors = vectorizer.transform(top_terms).toarray()\n",
        "        coherence, _ = pairwise_distances_argmin_min(term_vectors, doc_term_matrix)\n",
        "        coherence_scores.append(coherence.mean())\n",
        "    return LDA, vectorizer, sum(coherence_scores) / len(coherence_scores)"
      ],
      "metadata": {
        "id": "wwWZoOmoxUK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keyword Extraction using TF-IDF\n",
        "def keyword_extraction(text):\n",
        "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n",
        "    tfidf_scores = np.asarray(tfidf_matrix.mean(axis=0)).ravel().tolist()\n",
        "    tfidf_df = pd.DataFrame(list(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_scores)), columns=['Word', 'TF-IDF Score'])\n",
        "    return tfidf_df.sort_values(by='TF-IDF Score', ascending=False).head(10)"
      ],
      "metadata": {
        "id": "gWCBkLmTxXE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependency Parsing\n",
        "def dependency_parsing(text):\n",
        "    doc = nlp(text)\n",
        "    parsed = [(token.text, token.dep_, token.head.text) for token in doc]\n",
        "    return parsed, len(parsed), len(set([token.head.text for token in doc]))"
      ],
      "metadata": {
        "id": "LXoQkW0IxZHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Analysis\n",
        "def sentiment_analysis(text):\n",
        "    blob = TextBlob(text)\n",
        "    return blob.sentiment"
      ],
      "metadata": {
        "id": "CW9EGFlGxbN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Semantic Similarity\n",
        "def semantic_similarity(main_text, comparison_texts):\n",
        "    main_embedding = embedder.encode(main_text, convert_to_tensor=True)\n",
        "    comparison_embeddings = embedder.encode(comparison_texts, convert_to_tensor=True)\n",
        "    similarities = util.pytorch_cos_sim(main_embedding, comparison_embeddings)\n",
        "    return similarities.cpu().numpy().flatten()"
      ],
      "metadata": {
        "id": "637jXbe8xdPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Summarization\n",
        "def simple_summarization(text, num_sentences=3):\n",
        "    sentences = text.split('. ')\n",
        "    return '. '.join(sentences[:num_sentences])"
      ],
      "metadata": {
        "id": "lSqZDdpTxf9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment_value'] = df['combined_text'].apply(sentiment_analysis)\n",
        "df['summarization'] = df['combined_text'].apply(simple_summarization)"
      ],
      "metadata": {
        "id": "1N4EpS-Lxh6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize score function\n",
        "def normalize_score(score, min_val, max_val):\n",
        "    return (score - min_val) / (max_val - min_val)"
      ],
      "metadata": {
        "id": "PTmtnExvxj56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all features for the final rating\n",
        "def combine_results(text,named_entities,dependencies,sentiment_value,summarization):\n",
        "    score_orginality=[]\n",
        "    score_significance=[]\n",
        "    score_rigour=[]\n",
        "    preprocessed_text = text\n",
        "\n",
        "    # Named Entity Recognition (NER)\n",
        "    entities = named_entities\n",
        "\n",
        "    # Dependency Parsing\n",
        "    parsed, num_dependencies, num_heads = dependency_parsing(preprocessed_text)\n",
        "\n",
        "    # Topic Modeling\n",
        "    LDA_model, vectorizer, topic_coherence = topic_modeling(preprocessed_text)\n",
        "\n",
        "    # Keyword Extraction\n",
        "    tfidf_df = keyword_extraction(preprocessed_text)\n",
        "\n",
        "    # Sentiment Analysis\n",
        "    sentiment = sentiment_value\n",
        "\n",
        "    # Summarization\n",
        "    summary = summarization\n",
        "    summary_length = len(summary.split())\n",
        "\n",
        "    # Calculate raw scores\n",
        "    num_entities = len(entities)\n",
        "    top_keyword_score = tfidf_df.iloc[0]['TF-IDF Score'] if not tfidf_df.empty else 0\n",
        "\n",
        "    score_orginality.append(sentiment.polarity + (num_entities / 10) + (top_keyword_score / 100) + (topic_coherence / 5))\n",
        "    score_significance.append(sentiment.polarity + (num_dependencies / 50) + (num_heads / 10) + (summary_length / 50))\n",
        "    score_rigour.append(sentiment.polarity + (num_entities / 10) + (num_dependencies / 50) + (summary_length / 50))\n",
        "\n",
        "    # Normalize scores\n",
        "    # originality_score_normalized = normalize_score(originality_score, 29.912090614995588, 648.7089093079097)\n",
        "    # significance_score_normalized = normalize_score(significance_score, 34.76434749278499, 648.7089093079097)\n",
        "    # score_rigour = normalize_score(rigour_score, 61.518175529425534, 721.7422954996392)\n",
        "\n",
        "\n",
        "\n",
        "    final_rating = {\n",
        "        'originality_score': score_orginality,\n",
        "        'significance_score': score_significance,\n",
        "        'rigour_score': score_rigour\n",
        "    }\n",
        "\n",
        "    return final_rating"
      ],
      "metadata": {
        "id": "HYwDpbROxmPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each paper in parallel\n",
        "def process_paper(row):\n",
        "    # text = row['combined_text']\n",
        "    return combine_results(row['combined_text'],row['named_entities'],row['doc'],row['sentiment_value'],row['summarization'])"
      ],
      "metadata": {
        "id": "WMPfiBR8xpT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding results to the DataFrame\n",
        "ratings_df = pd.DataFrame(results_list)\n",
        "df[['originality_score','significance_score','rigour_score']] = ratings_df"
      ],
      "metadata": {
        "id": "t3abcNDxxxQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize scores to the 0-1 range\n",
        "min_orginality_score = df['originality_score'].min()\n",
        "max_score_orginality = df['originality_score'].max()\n",
        "df['orginality_normalized'] = [normalize_score(score,min_orginality_score,max_score_orginality) for score in df['originality_score']]"
      ],
      "metadata": {
        "id": "B-UH5ZB60Nm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rigour_score=[]\n",
        "for score in df['rigour_score']:\n",
        "  print(score[0])\n",
        "  rigour_score.append(score[0])\n",
        "df['rigour_score']=rigour_score\n",
        "# Normalize scores to the 0-1 range\n",
        "min_rigour_score = df['rigour_score'].min()\n",
        "max_rigour_orginality = df['rigour_score'].max()\n",
        "df['rigour_normalized'] = [normalize_score(score,min_orginality_score,max_score_orginality) for score in df['rigour_score']]"
      ],
      "metadata": {
        "id": "O12Q8Upz0Pci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "significance_score=[]\n",
        "for score in df['significance_score']:\n",
        "  significance_score.append(score[0])\n",
        "df['significance_score']=significance_score\n",
        "# Normalize scores to the 0-1 range\n",
        "min_significance_score = df['significance_score'].min()\n",
        "max_significance_orginality = df['significance_score'].max()\n",
        "df['significance_normalized'] = [normalize_score(score,min_orginality_score,max_score_orginality) for score in df['significance_score']]"
      ],
      "metadata": {
        "id": "sk2k444R0R15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine ratings based on normalized scores\n",
        "def determine_originality_rating(score):\n",
        "    if score > 0.2:\n",
        "        return 4\n",
        "    elif score > 0.15:\n",
        "        return 3\n",
        "    elif score > 0.1:\n",
        "        return 2\n",
        "    else:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "GKb-ukNT0Zab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine ratings based on normalized scores\n",
        "def determine_significance_rating(score):\n",
        "    if score > 0.15:\n",
        "        return 4\n",
        "    elif score > 0.1:\n",
        "        return 3\n",
        "    elif score > 0.05:\n",
        "        return 2\n",
        "    else:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "ttDWj5xK0b1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine ratings based on normalized scores\n",
        "def determine_rigour_rating(score):\n",
        "    if score > 0.4:\n",
        "        return 4\n",
        "    elif score > 0.3:\n",
        "        return 3\n",
        "    elif score > 0.2:\n",
        "        return 2\n",
        "    else:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "q009IxmH0dzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['originality']=df['orginality_normalized'].apply(determine_originality_rating)\n",
        "df['significance']=df['significance_normalized'].apply(determine_significance_rating)\n",
        "df['rigour']=df['rigour_normalized'].apply(determine_rigour_rating)"
      ],
      "metadata": {
        "id": "H7orcBGQ0g4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average of the three columns\n",
        "df['overall_rating'] = df[['originality', 'significance', 'rigour']].mean(axis=1).round().astype(int)\n",
        "# Display the DataFrame\n",
        "print(df['overall_rating'].value_counts())"
      ],
      "metadata": {
        "id": "Cfg1ifkp0j49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split data for Train and Test"
      ],
      "metadata": {
        "id": "MuSy0k2601nU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract TF-IDF features from the text\n",
        "vectorizer = TfidfVectorizer(max_features=1000)  # You can adjust max_features based on your dataset\n",
        "X = vectorizer.fit_transform(df['combined_text'])\n",
        "y = df['overall_rating']\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "LINMYKOX05nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "RuHLMDNawSFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "print(\"Evaluating Random Forest\")\n",
        "rf_model = RandomForestClassifier()\n",
        "cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='accuracy')\n",
        "print(f\"Cross-validation results for Random Forest:\")\n",
        "print(f\"Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\\n\")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred = cross_val_predict(rf_model, X, y, cv=5)\n",
        "print(f\"Results for Random Forest:\")\n",
        "print(\"Accuracy:\", accuracy_score(y, y_pred))\n",
        "print(classification_report(y, y_pred))\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "yvmk87bLQlAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter optimization using GridSearchCV for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, 30, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search_rf = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best parameters for Random Forest: \", grid_search_rf.best_params_)\n",
        "\n",
        "# Train the best model\n",
        "best_model_rf = grid_search_rf.best_estimator_\n",
        "best_model_rf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = best_model_rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(classification_report(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "GOcoVKGnMvZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance = best_model_rf.feature_importances_\n",
        "# Get feature names corresponding to TF-IDF features\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame to hold feature names and their importance scores\n",
        "# features_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importance})\n",
        "# features_df = features_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': feature_importance\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(importance_df['Feature'].head(20), importance_df['Importance'].head(20), color='royalblue')\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Top 20 Random Forest Feature Importances')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "# Print the feature importance\n",
        "print(importance_df.head(20))  # Show top 20 features for brevity"
      ],
      "metadata": {
        "id": "v0mOhxBdM1X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics for Random Forest\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "precision_rf = precision_score(y_test, y_pred_rf, average='weighted')\n",
        "recall_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
        "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
        "\n",
        "print(\"Random Forest Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_rf}\")\n",
        "print(f\"Precision: {precision_rf}\")\n",
        "print(f\"Recall: {recall_rf}\")\n",
        "print(f\"F1 Score: {f1_rf}\")"
      ],
      "metadata": {
        "id": "HEK6TbfeQfHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network LSTM"
      ],
      "metadata": {
        "id": "10byE2A9yOFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 1000\n",
        "max_len = 100\n",
        "tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
        "tokenizer.fit_on_texts(df['combined_text'].values)\n",
        "X_seq = tokenizer.texts_to_sequences(df['combined_text'].values)\n",
        "X_pad = pad_sequences(X_seq, maxlen=max_len)\n",
        "y_cat = pd.get_dummies(df['overall_rating']).values\n",
        "\n",
        "X_train_pad, X_test_pad, y_train_cat, y_test_cat = train_test_split(X_pad, y_cat, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define LSTM model\n",
        "def create_lstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_words, 128, input_length=max_len))\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(y_cat.shape[1], activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train and evaluate LSTM model\n",
        "print(\"Evaluating LSTM\")\n",
        "lstm_model = create_lstm_model()\n",
        "lstm_model.fit(X_train_pad, y_train_cat, epochs=10, batch_size=64, validation_split=0.1)\n",
        "lstm_y_pred = lstm_model.predict(X_test_pad)\n",
        "lstm_y_pred_classes = lstm_y_pred.argmax(axis=-1)\n",
        "lstm_y_test_classes = y_test_cat.argmax(axis=-1)\n",
        "\n",
        "print(\"Results for LSTM:\")\n",
        "print(\"Accuracy:\", accuracy_score(lstm_y_test_classes, lstm_y_pred_classes))\n",
        "print(classification_report(lstm_y_test_classes, lstm_y_pred_classes))\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "uTbF_w0ONJDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def permutation_importance(model, X_test_pad, y_test_cat, n_repeats=10):\n",
        "    baseline_score = accuracy_score(y_test_cat.argmax(axis=-1), model.predict(X_test_pad).argmax(axis=-1))\n",
        "    importance = np.zeros(X_test_pad.shape[1])\n",
        "    print(X_test_pad.shape[1])\n",
        "    print(n_repeats)\n",
        "    for i in range(X_test_pad.shape[1]):\n",
        "        score_diffs = []\n",
        "        for j in range(n_repeats):\n",
        "          print(j)\n",
        "          X_permuted = X_test_pad.copy()\n",
        "          np.random.shuffle(X_permuted[:, i])  # Shuffle the i-th feature (word position)\n",
        "          permuted_score = accuracy_score(y_test_cat.argmax(axis=-1), model.predict(X_permuted).argmax(axis=-1))\n",
        "          score_diffs.append(baseline_score - permuted_score)\n",
        "        importance[i] = np.mean(score_diffs)\n",
        "\n",
        "    return importance\n",
        "\n",
        "# Compute permutation importance\n",
        "perm_importance = permutation_importance(lstm_model, X_test_pad, y_test_cat)\n",
        "\n",
        "# Plot permutation importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(len(perm_importance)), perm_importance)\n",
        "plt.xlabel('Feature (Word Position)')\n",
        "plt.ylabel('Importance')\n",
        "plt.title('Permutation Feature Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yCeIMGizNQJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics for LSTM\n",
        "accuracy_lstm = accuracy_score(lstm_y_test_classes, lstm_y_pred_classes)\n",
        "precision_lstm = precision_score(lstm_y_test_classes, lstm_y_pred_classes, average='weighted')\n",
        "recall_lstm = recall_score(lstm_y_test_classes, lstm_y_pred_classes, average='weighted')\n",
        "f1_lstm = f1_score(lstm_y_test_classes, lstm_y_pred_classes, average='weighted')\n",
        "\n",
        "print(\"\\nLSTM Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_lstm}\")\n",
        "print(f\"Precision: {precision_lstm}\")\n",
        "print(f\"Recall: {recall_lstm}\")\n",
        "print(f\"F1 Score: {f1_lstm}\")"
      ],
      "metadata": {
        "id": "lHvuewl_Q0d-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Model"
      ],
      "metadata": {
        "id": "1D-A4fsE0-zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['combined_text'], df['overall_rating'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the data\n",
        "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "TntWZzRFNlLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)  # Ensure labels are float\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = TextDataset(train_encodings, train_labels.tolist())\n",
        "test_dataset = TextDataset(test_encodings, test_labels.tolist())\n",
        "\n",
        "# Load pre-trained model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "7AH5JcE3No3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=10,  # Set to 10 epochs\n",
        "    per_device_train_batch_size=16,  # Suitable batch size\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "1A4LtP0AN4tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "trainer.evaluate()\n",
        "\n",
        "# Make predictions\n",
        "predictions = trainer.predict(test_dataset)\n",
        "predicted_ratings = predictions.predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "zYYNIxzD_S2l",
        "outputId": "167e6914-82d1-451f-a3d0-1511f895fc71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "predicted_ratings = np.squeeze(predicted_ratings)\n",
        "predicted_labels = np.round(predicted_ratings)\n",
        "accuracy = np.mean(predicted_labels == test_labels.to_numpy())\n",
        "\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained('saved_model')\n",
        "tokenizer.save_pretrained('saved_model')"
      ],
      "metadata": {
        "id": "dZrxGr0EOBfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=10,  # Set to 10 epochs\n",
        "    per_device_train_batch_size=16,  # Suitable batch size\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,  # Adjust warmup steps if needed\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    learning_rate=3e-5,  # Experiment with different learning rates if needed\n",
        "    lr_scheduler_type='linear',  # Choose from 'linear', 'cosine', etc.\n",
        "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
        "    metric_for_best_model='eval_loss',  # Specify the metric to monitor\n",
        "    evaluation_strategy='steps', # Evaluate and save at the same time\n",
        "    save_strategy='steps' # Evaluate and save at the same time\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "f_N-NulxOF-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Trainer with early stopping\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "# Train and evaluate the model\n",
        "trainer.train()\n",
        "evaluation_results = trainer.evaluate()\n",
        "\n",
        "# Make predictions\n",
        "predictions = trainer.predict(test_dataset)\n",
        "predicted_ratings = predictions.predictions\n",
        "\n",
        "# Calculate accuracy\n",
        "predicted_ratings = np.squeeze(predicted_ratings)\n",
        "predicted_labels = np.round(predicted_ratings)\n",
        "accuracy = np.mean(predicted_labels == test_labels.to_numpy())\n",
        "\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained('saved_model_2')\n",
        "tokenizer.save_pretrained('saved_model_2')"
      ],
      "metadata": {
        "id": "4GeS3hQBOJGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained('saved_model_2')\n",
        "tokenizer = BertTokenizer.from_pretrained('saved_model_2')"
      ],
      "metadata": {
        "id": "Ud1hAw4GPXVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = df['combined_text'][0]\n",
        "inputs = tokenizer(sample_text, return_tensors='pt', truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "GdHKk0X-Pinz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predictions = outputs.logits\n",
        "predicted_rating = predictions.squeeze().item()\n",
        "predicted_label = round(predicted_rating)\n",
        "print(f\"Predicted rating: {predicted_rating}\")\n",
        "print(f\"Predicted label (rounded rating): {predicted_label}\")"
      ],
      "metadata": {
        "id": "umPZkYImOYQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass to get attention scores\n",
        "outputs = model(**inputs,output_attentions=True)\n",
        "attentions = outputs.attentions  # A list of attention scores from each layer\n",
        "\n",
        "# For simplicity, use the attention scores from the last layer\n",
        "last_layer_attentions = attentions[-1]  # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "# Aggregate across heads and tokens\n",
        "avg_attention = last_layer_attentions.mean(dim=1).squeeze(0).detach().numpy()\n",
        "\n",
        "# Plot attention heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(avg_attention, xticklabels=tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze().tolist()),\n",
        "            yticklabels=tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze().tolist()), cmap='viridis')\n",
        "plt.title('BERT Attention Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AlyyamOHOfUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# import numpy as np\n",
        "\n",
        "# Convert true labels to numpy array\n",
        "true_labels = test_labels.to_numpy().astype(int)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_BERT = accuracy_score(true_labels, predicted_label)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision_BERT = precision_score(true_labels, predicted_labels, average='binary')\n",
        "recall_BERT = recall_score(true_labels, predicted_labels, average='binary')\n",
        "f1_BERT = f1_score(true_labels, predicted_labels, average='binary')\n",
        "\n",
        "print(f'Precision: {precision * 100:.2f}%')\n",
        "print(f'Recall: {recall * 100:.2f}%')\n",
        "print(f'F1 Score: {f1 * 100:.2f}%')"
      ],
      "metadata": {
        "id": "3I2uamZBOncF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "models = ['Random Forest', 'LSTM', 'BERT']\n",
        "accuracy = [accuracy_rf, accuracy_lstm, accuracy_BERT ]\n",
        "precision = [precision_rf, precision_lstm, precision_BERT]\n",
        "recall = [precision_rf, precision_lstm, precision_BERT]\n",
        "f1_score = [precision_rf, precision_lstm, precision_BERT]\n",
        "\n",
        "x = np.arange(len(models))  # The label locations\n",
        "width = 0.2  # The width of the bars\n",
        "\n",
        "# Create the figure and the bar chart\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "plt.legend(loc='upper right', bbox_to_anchor=(1.15, 1))\n",
        "bars1 = ax.bar(x - 1.5*width, accuracy, width, label='Accuracy', color='mediumpurple')\n",
        "bars2 = ax.bar(x - 0.5*width, precision, width, label='Precision', color='cornflowerblue')\n",
        "bars3 = ax.bar(x + 0.5*width, recall, width, label='Recall', color='orange')\n",
        "bars4 = ax.bar(x + 1.5*width, f1_score, width, label='F1-Score', color='mediumseagreen')\n",
        "\n",
        "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_xlabel('Models')\n",
        "ax.set_ylabel('Scores (%)')\n",
        "ax.set_title('Model Performance Comparison')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models, rotation=25, ha='center')\n",
        "ax.legend()\n",
        "yticks = np.arange(0, 101, 25)  # Changed step size to 25\n",
        "ax.set_yticks(yticks)\n",
        "ax.set_yticklabels([f'{tick}%' for tick in yticks])\n",
        "\n",
        "# Attach a text label above each bar, displaying its height\n",
        "def add_labels(bars):\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.annotate(f'{height:.2f}',\n",
        "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "add_labels(bars1)\n",
        "add_labels(bars2)\n",
        "add_labels(bars3)\n",
        "add_labels(bars4)\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i72mk1u8SYtu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}